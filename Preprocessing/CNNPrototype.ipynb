{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/vision/3D_image_classification/#setup\n",
    "#tried to use above tutorial\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Lucas directory\n",
    "#C:/Users/Lucas/OneDrive - The University of Western Ontario/Documents/GitHub/SE4450-project-code/Preprocessing/Pxx-Wx-Sx_RF\n",
    "\n",
    "#Ian directory\n",
    "#H:/Documents/Github repositories/SE4450-project-code/Preprocessing/Pxx-Wx-Sx_RF\n",
    "\n",
    "dataFolder = 'H:/Documents/Github repositories/SE4450-project-code/Preprocessing/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the structure of the data to be fed into the neural network\\\n",
    "#Tried to convert lucas' code into functions and stuff, not totally sure of what the data structure is happening\n",
    "\n",
    "#Getting data from .mat file\n",
    "def readData(patientID):\n",
    "    rawMatFile = loadmat(dataFolder+patientID+'.mat')\n",
    "    return rawMatFile['rf1']\n",
    "\n",
    "#Grabbing frame pair and label data from excel\n",
    "def readExcel(filename, sheetname):\n",
    "    #Read in label data\n",
    "    dicLabeledData = pd.read_excel('Labels.xlsx', sheet_name=sheetname, engine='openpyxl')\n",
    "    \n",
    "    vecFirstFrames = dicLabeledData['Frame 1'].tolist()\n",
    "    vecSecondFrames = dicLabeledData['Frame 2'].tolist()\n",
    "    vecLabelValue = dicLabeledData['Label axial'].tolist()\n",
    "\n",
    "    matFramePairLabels = [vecFirstFrames,vecSecondFrames,vecLabelValue]\n",
    "\n",
    "    return matFramePairLabels\n",
    "\n",
    "def loadPairsAndLabels(patientID):\n",
    "    rawData = readData(patientID)\n",
    "    matFramePairLabels = readExcel('Labels.xlsx', patientID)\n",
    "\n",
    "    stackedLabeledPairs = []\n",
    "\n",
    "    for pairNum in range(len(matFramePairLabels[0])):\n",
    "        firstFrame = matFramePairLabels[0][pairNum]\n",
    "        secondFrame = matFramePairLabels[1][pairNum]\n",
    "        firstFrameData = rawData[:len(rawData)][:len(rawData[0])][firstFrame]\n",
    "        secondFrameData = rawData[:len(rawData)][:len(rawData[0])][secondFrame]\n",
    "        stackedFrame = np.stack((firstFrameData,secondFrameData), -1)\n",
    "\n",
    "    return stackedFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[ -81   55]\n  [  58  -15]\n  [  45  -32]\n  ...\n  [ -37    3]\n  [ -33   11]\n  [ -29    6]]\n\n [[   2  -53]\n  [ -23   35]\n  [  14  -37]\n  ...\n  [  18  -12]\n  [ -24    6]\n  [   4  -21]]\n\n [[-127  175]\n  [  -4    8]\n  [  64  -99]\n  ...\n  [ -19   -2]\n  [  15  -11]\n  [ -17    0]]\n\n ...\n\n [[ -18   -8]\n  [  46  -63]\n  [  -9   32]\n  ...\n  [   5  -26]\n  [  13  -18]\n  [  -1    5]]\n\n [[  15   17]\n  [ -17   16]\n  [ -11    0]\n  ...\n  [ -32   12]\n  [ -32    9]\n  [ -17  -14]]\n\n [[  -2   -9]\n  [ -19   -4]\n  [   9   -2]\n  ...\n  [  -1   -6]\n  [  -2    2]\n  [  -1    7]]]\n"
     ]
    }
   ],
   "source": [
    "#Read in label data\n",
    "stackedLabeledPairs = loadPairsAndLabels('P90-W0-S4')\n",
    "matFramePairLabels = readExcel('Labels.xlsx', 'P90-W0-S4')\n",
    "\n",
    "print(stackedLabeledPairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(volume):\n",
    "    #Adjust to normalize the scans to be between 0 and 1\n",
    "    min = -1000\n",
    "    max = 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process the scans.\n",
    "# Each scan is resized across height, width, and depth and rescaled.\n",
    "bad_pairs = np.array([process_scan(path) for path in abnormal_scan_paths])\n",
    "good_pairs = np.array([process_scan(path) for path in normal_scan_paths])\n",
    "\n",
    "# For the good pairs\n",
    "# assign 1, for the bad ones assign 0.\n",
    "bad_labels = np.array([0 for _ in range(len(bad_pairs))])\n",
    "good_labels = np.array([1 for _ in range(len(good_pairs))])\n",
    "\n",
    "# Split data in the ratio 70-30 for training and validation.\n",
    "x_train = np.concatenate((bad_pairs[:70], good_pairs[:70]), axis=0)\n",
    "y_train = np.concatenate((bad_labels[:70], good_labels[:70]), axis=0)\n",
    "x_val = np.concatenate((bad_pairs[70:], good_pairs[70:]), axis=0)\n",
    "y_val = np.concatenate((bad_labels[70:], good_labels[70:]), axis=0)\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (x_train.shape[0], x_val.shape[0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocessing(volume, label):\n",
    "    #\"\"\"Process training data by adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    #\"\"\"Process validation data by adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "\n",
    "batch_size = 2\n",
    "# Augment the on the fly during training.\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train))\n",
    "    .map(train_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_val))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d03271427f4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Build model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d03271427f4f>\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(width, height, depth)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#\"\"\"Build a 3D convolutional neural network model.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "def get_model(width=128, height=128, depth=64):\n",
    "    #\"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model.\n",
    "model = get_model(width=128, height=128, depth=64)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 100\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  }
 ]
}