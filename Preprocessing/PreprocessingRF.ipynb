{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages required for code functions\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformations  as tr\n",
    "import random as rand\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePairs(frames):\n",
    "    framePairs = [] #vector to hold image pairs\n",
    "\n",
    "    for x in range(len(frames)): #x represents the first frame in the pair\n",
    "        for y in range(len(frames)-3-x): #y represents second frame in the pair\n",
    "            pair = [(x+1),(x+y+4),np.stack((frames[x], frames[x+y+3]), axis=-1)] #generate pair\n",
    "            framePairs.append(pair)\n",
    "    return framePairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating translated pairs to increase the amount of good scans to train the model with\n",
    "def generateGoodData(goodData):\n",
    "    extraGood = []\n",
    "    for x in range(len(goodData)):\n",
    "        extra = translatePair(goodData[x][1],goodData[x][2],goodData[x][0],goodData[x][3],1,1)\n",
    "        extraGood = extraGood + extra\n",
    "    return extraGood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translates frame pairs in order to generate additional data\n",
    "def translatePair(frame1,frame2,patient,data,steps,label):\n",
    "\n",
    "    frame1 = frame1-1 #get first frame position from pair object\n",
    "    frame2 = frame2-1 #get second frame position from pair object\n",
    "    JAdd = data.shape[0] #Get size of original frame to keep translation size consistent\n",
    "    IAdd = data.shape[1] #Get size of original frame to keep translation size consistent\n",
    "\n",
    "    baseCut = 0.1 #use same base cut as is used in original frame generation\n",
    "    step = baseCut/steps #set step size to be as large as possible for given number of steps\n",
    "    pairs = [] #vector to hold newly generated pairs\n",
    "\n",
    "    #adjust how the frame is cropped from the original data to simulate translation\n",
    "    for x in range(-steps,steps+1): \n",
    "        for y in range(-steps,steps+1):\n",
    "            lowerI = int((baseCut+x*step)*256)\n",
    "            upperI = lowerI+IAdd\n",
    "            lowerJ = int((baseCut+y*step)*2064)\n",
    "            upperJ = lowerJ+JAdd\n",
    "\n",
    "            first = rawData[patient][1][lowerJ:upperJ,lowerI:upperI,frame1]\n",
    "            second = rawData[patient][1][lowerJ:upperJ,lowerI:upperI,frame2]\n",
    "\n",
    "            pairs.append([patient,frame1,frame2,np.stack((first,second), axis=-1),label]) #append translated pair to pairs vector\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicts frame pair fitness given a target number\n",
    "def predictLabel(number):\n",
    "    prediction = model.predict(np.expand_dims(np.expand_dims(x_test[number], axis=3),axis=0))[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory containing the RF scan data\n",
    "dataFolder = 'C:/Users/Lucas/OneDrive - The University of Western Ontario/Documents/GitHub/SE4450-project-code/Preprocessing/'\n",
    "\n",
    "#Vector to hold all training data sets\n",
    "rawData = []\n",
    "\n",
    "#Loading in the matlab file information\n",
    "rawMatFile = loadmat('P39-W2-S4')\n",
    "#Contains just the RF data pulled from the .mat file\n",
    "P39W2S4 = rawMatFile['rf1']\n",
    "rawData.append([0,P39W2S4])\n",
    "\n",
    "#repeat above steps for other RF files\n",
    "rawMatFile = loadmat('P39-W4-S6')\n",
    "P39W4S6 = rawMatFile['rf1']\n",
    "rawData.append([1,P39W4S6])\n",
    "rawMatFile = loadmat('P82-W0-S2')\n",
    "P82W0S2 = rawMatFile['rf1']\n",
    "# rawData.append(['P82W0S2',P82W0S2])\n",
    "rawMatFile = loadmat('P87-W0-S3')\n",
    "P87W0S3 = rawMatFile['rf1']\n",
    "rawData.append([2,P87W0S3])\n",
    "rawMatFile = loadmat('P87-W2-S4')\n",
    "P87W2S4 = rawMatFile['rf1']\n",
    "rawData.append([3,P87W2S4])\n",
    "rawMatFile = loadmat('P90-W0-S4')\n",
    "P90W0S4 = rawMatFile['rf1']\n",
    "rawData.append([4,P90W0S4])\n",
    "rawMatFile = loadmat('P94-W1-S3')\n",
    "P94W1S3 = rawMatFile['rf1']\n",
    "rawData.append([5,P94W1S3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#leave a slice of each edge of the scan from being black\n",
    "cut = 0.1\n",
    "\n",
    "#Assigning which pixels to cut\n",
    "lowerI = int(cut*256)\n",
    "upperI = int((1-cut)*256)\n",
    "lowerJ = int(cut*2064)\n",
    "upperJ = int((1-cut)*2064)\n",
    "\n",
    "allFrames = [] #vector to hold individual cropped frames\n",
    "\n",
    "for y in range(len(rawData)):\n",
    "    frames = []\n",
    "    for x in range(rawData[y][1].shape[2]): #range is how many frames there are in the set\n",
    "        frames.append(rawData[y][1][lowerJ:upperJ,lowerI:upperI,x]) #Take inner subset of frame to allow for translation\n",
    "    allFrames.append([rawData[y][0],frames])\n",
    "\n",
    "pairs = []\n",
    "for x in range(len(allFrames)):\n",
    "    pairs.append([allFrames[x][0],generatePairs(allFrames[x][1])]) #add generated pairs to list\n",
    "#print(pairs[5][1][0][2].shape)\n",
    "\n",
    "#PAIRS VARIABLE GUIDE\n",
    "#First value = Holds which patient set is being referred to\n",
    "#Second value = Holds either patient set name [0] or generated pairs for set\n",
    "#Third value = Holds all pairs for patient set\n",
    "#Fourth value = holds specific pairs for patient set\n",
    "#pairs[0][1][0][2].shape <- references the shape of the third pair of frames for a certain patients data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "998\n0 1 4 (1651, 205, 2)\n0 1 5 (1651, 205, 2)\n"
     ]
    }
   ],
   "source": [
    "#Create Vector of stacked pairs with frame number and patient ID for easy labeling\n",
    "data = []\n",
    "for x in range(len(pairs)):\n",
    "    for y in range(len(pairs[x][1])):\n",
    "        data.append([pairs[x][0],pairs[x][1][y][0],pairs[x][1][y][1],pairs[x][1][y][2]])\n",
    "\n",
    "print(len(data))\n",
    "for x in range(2):\n",
    "    print(data[x][0],data[x][1],data[x][2],data[x][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Lucas\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:308: UserWarning: Conditional Formatting extension is not supported and will be removed\n  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Read in label data\n",
    "DFP39W2S4 = pd.read_excel('Labels.xlsx', sheet_name='P39-W2-S4', engine='openpyxl')\n",
    "DFP39W4S6 = pd.read_excel('Labels.xlsx', sheet_name='P39-W4-S6', engine='openpyxl')\n",
    "DFP87W0S3 = pd.read_excel('Labels.xlsx', sheet_name='P87-W0-S3', engine='openpyxl')\n",
    "DFP87W2S4 = pd.read_excel('Labels.xlsx', sheet_name='P87-W2-S4', engine='openpyxl')\n",
    "DFP90W0S4 = pd.read_excel('Labels.xlsx', sheet_name='P90-W0-S4', engine='openpyxl')\n",
    "DFP94W1S3 = pd.read_excel('Labels.xlsx', sheet_name='P94-W1-S3', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "# + DFP82W0S2['Label axial'].tolist() wrong shape?\n",
    "#create labels for training data\n",
    "labels = DFP39W2S4['Label axial'].tolist() + DFP39W4S6['Label axial'].tolist()  + DFP87W0S3['Label axial'].tolist() + DFP87W2S4['Label axial'].tolist() + DFP90W0S4['Label axial'].tolist() + DFP94W1S3['Label axial'].tolist()\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying imported labels to imported data\n",
    "labeledData = []\n",
    "for x in range(len(labels)):\n",
    "    labeledData.append([data[x][0],data[x][1],data[x][2],data[x][3],labels[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "159\n",
      "839\n",
      "839\n",
      "839\n",
      "1678\n"
     ]
    }
   ],
   "source": [
    "goodData = []\n",
    "badData = []\n",
    "for x in range(len(labeledData)):\n",
    "    #Setting meh labels to 0\n",
    "    if labeledData[x][4] == 0.5:\n",
    "        labeledData[x][4] = 0\n",
    "\n",
    "    if labeledData[x][4] > 0:\n",
    "        goodData.append(labeledData[x])\n",
    "    else:\n",
    "        badData.append(labeledData[x])\n",
    "print(len(goodData))\n",
    "print(len(badData))\n",
    "# for x in range(len(goodData)):\n",
    "#     print(goodData[x][3].shape)\n",
    "goodData = generateGoodData(goodData) #generate more good data\n",
    "goodData = rand.sample(goodData,len(badData)) #sample from generated good data to get exactly even split\n",
    "print(len(goodData))\n",
    "print(len(badData))\n",
    "balancedData = goodData+badData\n",
    "print(len(balancedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning proper dimensions for input into model\n",
    "def train_preprocessing(data, label):\n",
    "    data = tf.expand_dims(data, axis=3)\n",
    "    return data, label\n",
    "    \n",
    "#Assigning proper dimensions for input into model\n",
    "def validation_preprocessing(data, label):\n",
    "    data = tf.expand_dims(data, axis=3)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data in the ratio 80-20 for training and validation.\n",
    "x_train, x_test, y_train, y_test = train_test_split([x[3] for x in balancedData], [x[4] for x in balancedData], test_size=0.20, random_state=42)\n",
    "\n",
    "# print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting testing and training data into tensors for input into model\n",
    "for x in range(len(x_train)):\n",
    "    x_train[x] = tf.convert_to_tensor(x_train[x], np.float32)\n",
    "\n",
    "#Converting testing and training data into tensors for input into model\n",
    "for x in range(len(x_test)):\n",
    "    x_test[x] = tf.convert_to_tensor(x_test[x], np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "batch_size = 2\n",
    "# Augment the on the fly during training.\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train))\n",
    "    .map(train_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_test))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"3dcnn\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 1651, 205, 2, 1)] 0         \n_________________________________________________________________\nconv3d (Conv3D)              (None, 1649, 203, 1, 64)  1216      \n_________________________________________________________________\nmax_pooling3d (MaxPooling3D) (None, 824, 101, 1, 64)   0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 824, 101, 1, 64)   256       \n_________________________________________________________________\nconv3d_1 (Conv3D)            (None, 822, 99, 1, 64)    36928     \n_________________________________________________________________\nmax_pooling3d_1 (MaxPooling3 (None, 411, 49, 1, 64)    0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 411, 49, 1, 64)    256       \n_________________________________________________________________\nconv3d_2 (Conv3D)            (None, 409, 47, 1, 128)   73856     \n_________________________________________________________________\nmax_pooling3d_2 (MaxPooling3 (None, 204, 23, 1, 128)   0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 204, 23, 1, 128)   512       \n_________________________________________________________________\nconv3d_3 (Conv3D)            (None, 202, 21, 1, 256)   295168    \n_________________________________________________________________\nmax_pooling3d_3 (MaxPooling3 (None, 101, 10, 1, 256)   0         \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 101, 10, 1, 256)   1024      \n_________________________________________________________________\nconv3d_4 (Conv3D)            (None, 99, 8, 1, 512)     1180160   \n_________________________________________________________________\nmax_pooling3d_4 (MaxPooling3 (None, 49, 4, 1, 512)     0         \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 49, 4, 1, 512)     2048      \n_________________________________________________________________\nglobal_average_pooling3d (Gl (None, 512)               0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               262656    \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 513       \n=================================================================\nTotal params: 1,854,593\nTrainable params: 1,852,545\nNon-trainable params: 2,048\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Constructing the 3D convolutional neural network\n",
    "def get_model(width, height, depth):\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "    \n",
    "    x = layers.Conv3D(filters=64, kernel_size=(3,3,2), activation=\"relu\")(inputs) #Convolutional layer which is automatically trained to detect desired features\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x) #Pooling layer which reduces the dimensionality of the RF data to allow for faster processing in further layers\n",
    "    x = layers.BatchNormalization()(x) #Normalizes the data post convolution and pooling to allow for faster training\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=(3,3,1), activation=\"relu\")(x) #Convolutional layer which is automatically trained to detect desired features\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x) #Pooling layer which reduces the dimensionality of the RF data to allow for faster processing in further layers\n",
    "    x = layers.BatchNormalization()(x) #Normalizes the data post convolution and pooling to allow for faster training\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=(3,3,1), activation=\"relu\")(x) #Convolutional layer which is automatically trained to detect desired features\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x) #Pooling layer which reduces the dimensionality of the RF data to allow for faster processing in further layers\n",
    "    x = layers.BatchNormalization()(x) #Normalizes the data post convolution and pooling to allow for faster training\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=(3,3,1), activation=\"relu\")(x) #Convolutional layer which is automatically trained to detect desired features\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x) #Pooling layer which reduces the dimensionality of the RF data to allow for faster processing in further layers\n",
    "    x = layers.BatchNormalization()(x) #Normalizes the data post convolution and pooling to allow for faster training\n",
    "\n",
    "    x = layers.Conv3D(filters=512, kernel_size=(3,3,1), activation=\"relu\")(x) #Convolutional layer which is automatically trained to detect desired features\n",
    "    x = layers.MaxPool3D(pool_size=(2,2,1))(x) #Pooling layer which reduces the dimensionality of the RF data to allow for faster processing in further layers\n",
    "    x = layers.BatchNormalization()(x) #Normalizes the data post convolution and pooling to allow for faster training\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x) #Calculates average output of each feature map in previous layers in preperation for final classification\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x) #Neurons in layer are connected to each previous neuron, performs matrix-vector multiplication\n",
    "    x = layers.Dropout(0.3)(x) #Helps to prevent overfitting\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x) #defining final neuron for classification\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model.\n",
    "model = get_model(width=1651, height=205, depth=2)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 50\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This model is 0.00 percent confident that frame pair is a bad pair\nThis model is 100.00 percent confident that frame pair is an optimal pair\n1\n"
     ]
    }
   ],
   "source": [
    "#Code to test the accuracy of the trained network\n",
    "model.load_weights(\"3d_image_classification.h5\")\n",
    "toTest = 85 #frame number in y_test to check\n",
    "\n",
    "prediction = model.predict(np.expand_dims(np.expand_dims(x_test[toTest], axis=3),axis=0))[0]\n",
    "scores = [1 - prediction[0], prediction[0]]\n",
    "\n",
    "class_names = [\"a bad pair\", \"an optimal pair\"]\n",
    "#print prediction in easy to view format\n",
    "for score, name in zip(scores, class_names):\n",
    "    print(\n",
    "        \"This model is %.2f percent confident that frame pair is %s\"\n",
    "        % ((100 * score), name)\n",
    "    )\n",
    "#print actual system label for comparison\n",
    "print(y_test[toTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "336\n",
      "0\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "#Running batch predictions to test model accuracy\n",
    "predictions = []\n",
    "print(len(y_test))\n",
    "for x in range(len(y_test)):\n",
    "    if (x%25==0):\n",
    "        print(x)\n",
    "    if (predictLabel(x)>0.5):\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "159 5 14 158\n0.9434523809523809\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(tn, fp, fn, tp) #print prediction results for confusion matrix\n",
    "print((tn+tp)/336) #print overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[159,   5],\n",
       "       [ 14, 158]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions) #print above results in traditional confusion matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BELOW ARE FUNCTIONS FOR FINAL PREDICTOR CLASS\n",
    "##WRITTEN HERE FIRST FOR SIMPLICITY\n",
    "##COMMENTED VERSIONS IN Predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FUNCTION 1 for final predictions\n",
    "def preprocessData(data):\n",
    "    height = data.shape[0]\n",
    "    width =  data.shape[1]\n",
    "    lowerH = int((height-1651)/2)\n",
    "    upperH = lowerH+1651\n",
    "    lowerW = int((width-205)/2)\n",
    "    upperW = lowerW + 205\n",
    "\n",
    "    frames = []\n",
    "    for x in range(data.shape[2]): #range is how many frames there are in the set\n",
    "        frames.append(data[lowerH:upperH,lowerW:upperW,x]) #cutdata to match size expected by model\n",
    "\n",
    "    pairs = generatePairs(frames)\n",
    "    for x in range(len(pairs)):\n",
    "        pairs[x][2] = tf.convert_to_tensor(pairs[x][2], np.float32)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FUCNTION 2 for final predicitons\n",
    "def predict(processed):\n",
    "    predictions = []\n",
    "    print(len(processed))\n",
    "    for x in range(len(processed)):\n",
    "        probability = model.predict(np.expand_dims(np.expand_dims(processed[x][2], axis=3),axis=0))[0]\n",
    "        if (x%50==0):\n",
    "            print(x)\n",
    "        if (probability>0.5):\n",
    "            predictions.append([1,probability])\n",
    "        else:\n",
    "            predictions.append([0,probability])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FUNCTION 3 for final predictions\n",
    "def formatOutput(processed,predictions):\n",
    "    output = []\n",
    "    for x in range(len(predictions)):\n",
    "        output.append([processed[x][0],processed[x][1],predictions[x][1][0]])\n",
    "    output.sort(key=lambda a: a[2],reverse=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "210\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "##FINAL PREDICTION\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "rawMatFile = loadmat('P39-W2-S4')\n",
    "P39W2S4 = rawMatFile['rf1']\n",
    "model.load_weights(\"3d_image_classification.h5\")\n",
    "processed = preprocessData(P39W2S4)\n",
    "predictions = predict(processed)\n",
    "output = formatOutput(processed,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[17, 22, 0.7949978]\n[13, 22, 0.70975673]\n[15, 23, 0.7079406]\n[14, 23, 0.64001644]\n[16, 22, 0.63846636]\n"
     ]
    }
   ],
   "source": [
    "##Outputs top five frame pairs most likely to be good and how likely\n",
    "print(output[0])\n",
    "print(output[1])\n",
    "print(output[2])\n",
    "print(output[3])\n",
    "print(output[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}